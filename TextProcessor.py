# Imports

# from re import sub
# from nltk.corpus import stopwords
# from nltk.tokenize import sent_tokenize
# from keras.preprocessing.text import Tokenizer

# Methods
def process_text(text: str):
    # TODO: COMPLETE
    # stop_words = set(stopwords.words("english"))
    # text = re.sub("\W+", " ", text)                                             # Removes everything except alphabets
    # text = " ".join([word for word in text.split() if word not in stop_words])  # Removes stopwords + spaces
    return text